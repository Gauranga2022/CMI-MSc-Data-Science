{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmGlRG6lsgTw"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [datasets]3/4\u001b[0m [datasets]\n",
      "Successfully installed datasets-3.5.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModel, BertModel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "np.random.seed(229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingore warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNRKcAk2LEXL"
   },
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1745839893364,
     "user": {
      "displayName": "Gauranga Kumar Baishya",
      "userId": "09042457344042071897"
     },
     "user_tz": -330
    },
    "id": "3z1XFX0ckgb5"
   },
   "outputs": [],
   "source": [
    "# read/prep data\n",
    "data = pd.read_csv(\"../data/tokenized_reviews.csv\")\n",
    "data = data.dropna()\n",
    "data[\"quote\"] = data[\"quote\"].astype(int)\n",
    "data[\"tokenized_words\"] = data[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710118, 14) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745839893431,
     "user": {
      "displayName": "Gauranga Kumar Baishya",
      "userId": "09042457344042071897"
     },
     "user_tz": -330
    },
    "id": "U-B3l2kAkisk"
   },
   "outputs": [],
   "source": [
    "# Split text and numerical features\n",
    "# 85% train / 15% test\n",
    "X = data.drop(columns=[\"popular\"])\n",
    "y = data[\"popular\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1453600, 13), (1453600,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling majority class\n",
    "majority_idx = y_train[y_train == 0].index\n",
    "minority_idx = y_train[y_train == 1].index\n",
    "drop_idx = np.random.default_rng(seed=229).choice(\n",
    "    majority_idx, len(majority_idx) - len(minority_idx), replace=False\n",
    ")\n",
    "X_train = X_train.drop(index=drop_idx)\n",
    "y_train = y_train.drop(index=drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((431298, 13), (431298,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>rating_diff</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>pct_verbs</th>\n",
       "      <th>pct_nouns</th>\n",
       "      <th>pct_adj</th>\n",
       "      <th>quote</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1519074</th>\n",
       "      <td>267</td>\n",
       "      <td>1280</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.93</td>\n",
       "      <td>1453</td>\n",
       "      <td>4.269787</td>\n",
       "      <td>15.793478</td>\n",
       "      <td>0.185822</td>\n",
       "      <td>0.255334</td>\n",
       "      <td>0.160358</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.014139</td>\n",
       "      <td>book review start saying read good book jeffer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428448</th>\n",
       "      <td>350</td>\n",
       "      <td>1027</td>\n",
       "      <td>5</td>\n",
       "      <td>0.18</td>\n",
       "      <td>96</td>\n",
       "      <td>3.958333</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372750</td>\n",
       "      <td>hobbes calvin good companion hard time even th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_reviews  days_since_review  user_rating  rating_diff  num_words  \\\n",
       "1519074           267               1280            1        -2.93       1453   \n",
       "428448            350               1027            5         0.18         96   \n",
       "\n",
       "         avg_word_len  avg_sent_len  pct_verbs  pct_nouns   pct_adj  quote  \\\n",
       "1519074      4.269787     15.793478   0.185822   0.255334  0.160358      1   \n",
       "428448       3.958333      9.600000   0.208333   0.208333  0.197917      0   \n",
       "\n",
       "         sentiment                                    tokenized_words  \n",
       "1519074  -0.014139  book review start saying read good book jeffer...  \n",
       "428448    0.372750  hobbes calvin good companion hard time even th...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUp59upgr-ly"
   },
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical columns\n",
    "numerical_cols = [col for col in X_train.columns if col != \"tokenized_words\"]\n",
    "\n",
    "# Custom transformer to get BERT embeddings\n",
    "class BertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='bert-base-uncased', device=None, batch_size=16):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        import torch\n",
    "        from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(self.model_name)\n",
    "        model = BertModel.from_pretrained(self.model_name)\n",
    "\n",
    "        # Device detection\n",
    "        if not self.device:\n",
    "            if torch.backends.mps.is_available():\n",
    "                self.device = \"mps\"\n",
    "            elif torch.cuda.is_available():\n",
    "                self.device = \"cuda\"\n",
    "            else:\n",
    "                self.device = \"cpu\"\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        X = X.squeeze()\n",
    "        all_embeddings = []\n",
    "\n",
    "        # Process in batches\n",
    "        for i in range(0, len(X), self.batch_size):\n",
    "            batch = list(X[i:i + self.batch_size])\n",
    "            tokens = tokenizer(batch, padding=True, truncation=True,\n",
    "                               max_length=128, return_tensors='pt')\n",
    "            tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokens)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(cls_embeddings)\n",
    "\n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31708,
     "status": "ok",
     "timestamp": 1745839980085,
     "user": {
      "displayName": "Gauranga Kumar Baishya",
      "userId": "09042457344042071897"
     },
     "user_tz": -330
    },
    "id": "FcD-6lSJj94Y",
    "outputId": "cb707216-6d17-46db-b84d-571f5f93a327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Logistic Regression-BERT\n",
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................................classifier__C=10; total time=69.0min\n",
      "[CV] END ....................................classifier__C=1; total time=68.8min\n",
      "[CV] END .................................classifier__C=0.01; total time=53.4min\n",
      "[CV] END ................................classifier__C=0.001; total time=42.7min\n",
      "\n",
      "Training completed in: 17490.19 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nLogistic Regression-BERT\")\n",
    "start_time = time.time()\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"bert\", BertVectorizer(), \"tokenized_words\"),\n",
    "    (\"num\", StandardScaler(), numerical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "bert_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='saga',\n",
    "        max_iter=5000,\n",
    "        random_state=229,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "parameters = {\"classifier__C\": [10, 1, 0.01, 0.001]}\n",
    "\n",
    "gs_bert_pipe = GridSearchCV(\n",
    "    estimator=bert_pipe,\n",
    "    param_grid=parameters,\n",
    "    cv=ShuffleSplit(n_splits=1, test_size=0.15, random_state=229), n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gs_bert_pipe.fit(X_train, y_train)\n",
    "print(f\"\\nTraining completed in: {time.time() - start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([3826.54654598, 3791.18984723, 2878.24719524, 2258.39360213]), 'std_fit_time': array([0., 0., 0., 0.]), 'mean_score_time': array([315.55065107, 335.74843287, 328.23438287, 304.49698973]), 'std_score_time': array([0., 0., 0., 0.]), 'param_classifier__C': masked_array(data=[10.0, 1.0, 0.01, 0.001],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value=1e+20), 'params': [{'classifier__C': 10}, {'classifier__C': 1}, {'classifier__C': 0.01}, {'classifier__C': 0.001}], 'split0_test_score': array([0.6903161 , 0.69039338, 0.69045521, 0.68962053]), 'mean_test_score': array([0.6903161 , 0.69039338, 0.69045521, 0.68962053]), 'std_test_score': array([0., 0., 0., 0.]), 'rank_test_score': array([3, 2, 1, 4], dtype=int32)}\n",
      "{'classifier__C': 0.01}\n",
      "\n",
      "Best model saved as 'logistic_bert_model_cde.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(gs_bert_pipe.cv_results_)\n",
    "print(gs_bert_pipe.best_params_)\n",
    "\n",
    "# save the best model with pickle\n",
    "with open(\"./logistic_bert_model_cde.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gs_bert_pipe.best_estimator_, f)\n",
    "\n",
    "print(\"\\nBest model saved as 'logistic_bert_model_cde.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero coefficients: 780\n",
      "Top 10 embedding dimensions contributing to prediction: [768 770 771 615 449 130 318 453 442  15]\n",
      "Corresponding weights: [ 0.74253218 -0.72597141  0.64425828  0.29991791  0.27611394 -0.27512348\n",
      " -0.26906531 -0.2504998   0.24097109  0.23963258]\n"
     ]
    }
   ],
   "source": [
    "# Get the best trained pipeline\n",
    "best_model = gs_bert_pipe.best_estimator_\n",
    "\n",
    "# Extract coefficients from the logistic regression model\n",
    "coef = best_model.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Number of non-zero coefficients\n",
    "num_nonzero = np.sum(np.abs(coef) > 0)\n",
    "print(f\"Number of non-zero coefficients: {num_nonzero}\")\n",
    "\n",
    "# Get top contributing embedding dimensions\n",
    "top_dims = np.argsort(np.abs(coef))[::-1][:10]\n",
    "print(\"Top 10 embedding dimensions contributing to prediction:\", top_dims)\n",
    "print(\"Corresponding weights:\", coef[top_dims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = gs_bert_pipe.predict(X_test)\n",
    "predictions = list(map(round,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[157691  60771]\n",
      " [ 12901  25155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.72      0.81    218462\n",
      "           1       0.29      0.66      0.41     38056\n",
      "\n",
      "    accuracy                           0.71    256518\n",
      "   macro avg       0.61      0.69      0.61    256518\n",
      "weighted avg       0.83      0.71      0.75    256518\n",
      "\n",
      "Specificity : 0.7218234750208274\n",
      "ROC-AUC : 0.6914115272938908\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Specificity :\", tn/(fp+tn))\n",
    "print(\"ROC-AUC :\", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNDiHvIUAOhTS4xOXUPN/7t",
   "collapsed_sections": [
    "tmGlRG6lsgTw",
    "RNRKcAk2LEXL",
    "raupEfc6ruUV",
    "DUp59upgr-ly"
   ],
   "mount_file_id": "1wiQz0_oxrtplB961FuEzA_XkPFFkAdxl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
