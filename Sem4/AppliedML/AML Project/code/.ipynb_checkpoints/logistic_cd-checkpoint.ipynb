{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["tmGlRG6lsgTw","RNRKcAk2LEXL","raupEfc6ruUV","DUp59upgr-ly"],"mount_file_id":"1wiQz0_oxrtplB961FuEzA_XkPFFkAdxl","authorship_tag":"ABX9TyNDiHvIUAOhTS4xOXUPN/7t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##### Libraries"],"metadata":{"id":"tmGlRG6lsgTw"}},{"cell_type":"code","source":["import sys\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from gensim.models import Word2Vec\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# from gensim.models import Word2Vec\n","from sklearn.base import BaseEstimator, TransformerMixin\n","np.set_printoptions(threshold=sys.maxsize)"],"metadata":{"id":"n_AlSjclkUss","executionInfo":{"status":"ok","timestamp":1745839893116,"user_tz":-330,"elapsed":22,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["##### Splitting"],"metadata":{"id":"RNRKcAk2LEXL"}},{"cell_type":"code","source":["# read/prep data\n","dat = pd.read_csv(\"/content/drive/MyDrive/AML Project/nlp/data/tokenized_reviews_10k.csv\")\n","dat = dat.dropna()\n","dat[\"quote\"] = dat[\"quote\"].astype(int)\n","dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))"],"metadata":{"id":"3z1XFX0ckgb5","executionInfo":{"status":"ok","timestamp":1745839893364,"user_tz":-330,"elapsed":247,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# 85% train / 15% test\n","X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]),\n","                                                    dat[\"popular\"],\n","                                                    test_size = 0.15,\n","                                                    random_state = 229)"],"metadata":{"id":"U-B3l2kAkisk","executionInfo":{"status":"ok","timestamp":1745839893431,"user_tz":-330,"elapsed":4,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# undersample train set\n","majority_size = len(y_train[y_train==0])\n","minority_size = len(y_train[y_train==1])\n","majority_indices = y_train[y_train==0].index\n","rng = np.random.default_rng(seed=229)\n","drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n","X_train = X_train.drop(drop_indices)\n","y_train = y_train.drop(drop_indices)"],"metadata":{"id":"AlMGOkKJkktj","executionInfo":{"status":"ok","timestamp":1745839893432,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["##### Bag Of Words"],"metadata":{"id":"raupEfc6ruUV"}},{"cell_type":"code","source":["# BAG OF WORDS\n","print(\"\\n\\nLOGISTIC REGRESSION BOW\")\n","\n","# pipeline\n","bow_pipe = make_pipeline(\n","    ColumnTransformer(remainder='passthrough',\n","                      transformers=[('countvectorizer',\n","                                     CountVectorizer(),\n","                                     'tokenized_words')]),\n","    StandardScaler(with_mean=False),\n","    LogisticRegression(penalty='l2',\n","                       solver='saga',\n","                       max_iter=1000,\n","                       random_state=229,\n","                       n_jobs=-1))\n","\n","# parameters to try\n","parameters = {\n","    'columntransformer__countvectorizer__max_features': (10000,50000),\n","    'logisticregression__C': (10, 1, 0.01, 0.001)\n","}\n","\n","# perform validation\n","gs_bow_pipe = GridSearchCV(bow_pipe,\n","                           parameters,\n","                           cv=ShuffleSplit(n_splits=1,\n","                                           test_size=0.13,\n","                                           random_state=229))\n","gs_bow_pipe.fit(X_train, y_train)\n","print(gs_bow_pipe.cv_results_)\n","print(gs_bow_pipe.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FUdfW-T-koqQ","executionInfo":{"status":"ok","timestamp":1745839948204,"user_tz":-330,"elapsed":54774,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"606cbf19-c1ea-4553-fabc-1e6ea10f3e0a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","LOGISTIC REGRESSION BOW\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'mean_fit_time': array([6.21396136, 7.92208624, 6.23531294, 1.83750844, 8.77006698,\n","       8.13105154, 7.71030831, 3.65900946]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([0.0433259 , 0.04240632, 0.04928327, 0.0433476 , 0.04447103,\n","       0.07193708, 0.0448544 , 0.07889438]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'param_columntransformer__countvectorizer__max_features': masked_array(data=[10000, 10000, 10000, 10000, 50000, 50000, 50000, 50000],\n","             mask=[False, False, False, False, False, False, False, False],\n","       fill_value=999999), 'param_logisticregression__C': masked_array(data=[10.0, 1.0, 0.01, 0.001, 10.0, 1.0, 0.01, 0.001],\n","             mask=[False, False, False, False, False, False, False, False],\n","       fill_value=1e+20), 'params': [{'columntransformer__countvectorizer__max_features': 10000, 'logisticregression__C': 10}, {'columntransformer__countvectorizer__max_features': 10000, 'logisticregression__C': 1}, {'columntransformer__countvectorizer__max_features': 10000, 'logisticregression__C': 0.01}, {'columntransformer__countvectorizer__max_features': 10000, 'logisticregression__C': 0.001}, {'columntransformer__countvectorizer__max_features': 50000, 'logisticregression__C': 10}, {'columntransformer__countvectorizer__max_features': 50000, 'logisticregression__C': 1}, {'columntransformer__countvectorizer__max_features': 50000, 'logisticregression__C': 0.01}, {'columntransformer__countvectorizer__max_features': 50000, 'logisticregression__C': 0.001}], 'split0_test_score': array([0.57783019, 0.57783019, 0.59198113, 0.62735849, 0.59433962,\n","       0.59433962, 0.59433962, 0.61084906]), 'mean_test_score': array([0.57783019, 0.57783019, 0.59198113, 0.62735849, 0.59433962,\n","       0.59433962, 0.59433962, 0.61084906]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([7, 7, 6, 1, 3, 3, 3, 2], dtype=int32)}\n","{'columntransformer__countvectorizer__max_features': 10000, 'logisticregression__C': 0.001}\n"]}]},{"cell_type":"code","source":["# predict\n","predictions = gs_bow_pipe.predict(X_test)\n","predictions = list(map(round,predictions))\n","print(classification_report(y_test, predictions))\n","print(confusion_matrix(y_test, predictions))\n","print(roc_auc_score(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"967VNz3bs9Xp","executionInfo":{"status":"ok","timestamp":1745839948326,"user_tz":-330,"elapsed":120,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"4c12d7f4-25bb-4f63-e73b-f3e70cfbe0ea"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.87      0.70      0.78      1214\n","           1       0.30      0.55      0.39       286\n","\n","    accuracy                           0.67      1500\n","   macro avg       0.59      0.62      0.58      1500\n","weighted avg       0.76      0.67      0.70      1500\n","\n","[[855 359]\n"," [130 156]]\n","0.6248689531226599\n"]}]},{"cell_type":"code","source":["# feature importance\n","coefficients = gs_bow_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n","num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n","sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n","print(len(sorted_ind))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"iMbqyaSGr2s7","executionInfo":{"status":"ok","timestamp":1745839948370,"user_tz":-330,"elapsed":30,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"6e45d6ce-bc4b-4a18-83f7-dd4b6adc6d0e"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["10012\n"]}]},{"cell_type":"code","source":["# Access the CountVectorizer from the ColumnTransformer\n","feature_names = gs_bow_pipe.best_estimator_.named_steps['columntransformer'].named_transformers_['countvectorizer'].get_feature_names_out()\n","sorted_ind_filtered = sorted_ind[sorted_ind < len(feature_names)]\n","important_features = np.take(feature_names, sorted_ind_filtered.tolist())\n","print('important-features:', important_features[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ijPnAp1nr7bI","executionInfo":{"status":"ok","timestamp":1745839948376,"user_tz":-330,"elapsed":5,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"effa9e2b-4367-4bab-d44f-d287a4fac1ac"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["important-features: ['channel' 'arc' 'graphic' 'publish' 'glad' 'already' 'finish' 'review'\n"," 'alright' 'improved']\n"]}]},{"cell_type":"markdown","source":["##### TF-IDF"],"metadata":{"id":"DUp59upgr-ly"}},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"FcD-6lSJj94Y","executionInfo":{"status":"ok","timestamp":1745839980085,"user_tz":-330,"elapsed":31708,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"cb707216-6d17-46db-b84d-571f5f93a327"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","LOGISTIC REGRESSION TF-IDF\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'mean_fit_time': array([7.09153724, 9.54863071, 7.14545059, 4.34465909]), 'std_fit_time': array([0., 0., 0., 0.]), 'mean_score_time': array([0.05497622, 0.0577662 , 0.07501698, 0.04515982]), 'std_score_time': array([0., 0., 0., 0.]), 'param_logisticregression__C': masked_array(data=[10.0, 1.0, 0.01, 0.001],\n","             mask=[False, False, False, False],\n","       fill_value=1e+20), 'params': [{'logisticregression__C': 10}, {'logisticregression__C': 1}, {'logisticregression__C': 0.01}, {'logisticregression__C': 0.001}], 'split0_test_score': array([0.57075472, 0.57075472, 0.57311321, 0.58254717]), 'mean_test_score': array([0.57075472, 0.57075472, 0.57311321, 0.58254717]), 'std_test_score': array([0., 0., 0., 0.]), 'rank_test_score': array([3, 3, 2, 1], dtype=int32)}\n","{'logisticregression__C': 0.001}\n"]}],"source":["# TF-IDF\n","print(\"\\n\\nLOGISTIC REGRESSION TF-IDF\")\n","\n","# pipeline\n","tf_pipe = make_pipeline(\n","    ColumnTransformer(remainder='passthrough',\n","                      transformers=[('tfidfvectorizer',\n","                                     TfidfVectorizer(),\n","                                     'tokenized_words')]),\n","    StandardScaler(with_mean=False),\n","    LogisticRegression(penalty='l2',\n","                       solver='saga',\n","                       max_iter=1000,\n","                       random_state=229,\n","                       n_jobs=-1))\n","\n","# parameters to try\n","parameters = {\n","    'logisticregression__C': (10, 1, 0.01, 0.001)\n","}\n","\n","# perform validation\n","gs_tf_pipe = GridSearchCV(tf_pipe,\n","                           parameters,\n","                           cv=ShuffleSplit(n_splits=1,\n","                                           test_size=0.13,\n","                                           random_state=229))\n","gs_tf_pipe.fit(X_train, y_train)\n","print(gs_tf_pipe.cv_results_)\n","print(gs_tf_pipe.best_params_)"]},{"cell_type":"code","source":["# predict\n","predictions = gs_tf_pipe.predict(X_test)\n","predictions = list(map(round,predictions))\n","print(classification_report(y_test, predictions))\n","print(confusion_matrix(y_test, predictions))\n","print(roc_auc_score(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AS3GV2sDtRkc","executionInfo":{"status":"ok","timestamp":1745839980090,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"f0590656-5748-46a5-8db2-01eff6811ebe"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.58      0.70      1214\n","           1       0.27      0.65      0.38       286\n","\n","    accuracy                           0.59      1500\n","   macro avg       0.57      0.61      0.54      1500\n","weighted avg       0.76      0.59      0.64      1500\n","\n","[[702 512]\n"," [100 186]]\n","0.6143016785520904\n"]}]},{"cell_type":"code","source":["# feature importance\n","coefficients = gs_tf_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n","num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n","sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n","print(len(sorted_ind))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"9_9xo-vjsGDy","executionInfo":{"status":"ok","timestamp":1745839980094,"user_tz":-330,"elapsed":5,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"95ea3600-1da2-40df-8815-2cfc929867df"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["23371\n"]}]},{"cell_type":"code","source":["# Access the CountVectorizer from the ColumnTransformer\n","feature_names = gs_bow_pipe.best_estimator_.named_steps['columntransformer'].named_transformers_['countvectorizer'].get_feature_names_out()\n","sorted_ind_filtered = sorted_ind[sorted_ind < len(feature_names)]\n","important_features = np.take(feature_names, sorted_ind_filtered.tolist())\n","print('important-features:', important_features[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0IjT4I_msINO","executionInfo":{"status":"ok","timestamp":1745839980098,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"d5fd20b9-6d8d-45ed-8a83-c8562f001eb1"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["important-features: ['forbidden' 'brady' 'talented' 'castle' 'heed' 'sadism' 'awry' 'soldier'\n"," 'alluring' 'pick']\n"]}]},{"cell_type":"markdown","source":["##### Word2vec"],"metadata":{"id":"m-HwinqBsOiQ"}},{"cell_type":"markdown","source":["Dependencies change for word2vec"],"metadata":{"id":"1RKEu8YN8Rg8"}},{"cell_type":"code","source":["# Uninstall to avoid binary incompatibility\n","!pip uninstall -y numpy gensim\n","\n","# Install compatible versions\n","!pip install numpy==1.26.4 gensim --quiet\n","\n","import numpy\n","import gensim\n","print(f\"NumPy: {numpy.__version__}\\nGensim: {gensim.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"KB32c0jF6GT1","executionInfo":{"status":"ok","timestamp":1745839990526,"user_tz":-330,"elapsed":10427,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"dd133879-7026-44be-e5c3-64389b81b6df"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Successfully uninstalled numpy-1.26.4\n","Found existing installation: gensim 4.3.3\n","Uninstalling gensim-4.3.3:\n","  Successfully uninstalled gensim-4.3.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mNumPy: 1.26.4\n","Gensim: 4.3.3\n"]}]},{"source":["print(\"\\n\\nLOGISTIC REGRESSION WORD2VEC\")\n","\n","# Custom transformer for Word2Vec averaging\n","class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, vector_size=100, window=5, min_count=1, workers=1, seed=229):\n","        self.vector_size = vector_size\n","        self.window = window\n","        self.min_count = min_count\n","        self.workers = workers\n","        self.seed = seed\n","        self.model = None\n","\n","    def fit(self, X, y=None):\n","        # X['tokenized_words'] should be a list of tokens per review\n","        sentences = X['tokenized_words'].apply(lambda x: x.split()).tolist() # Split the tokenized words into a list of words\n","        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window,\n","                              min_count=self.min_count, workers=self.workers, seed=self.seed)\n","        return self\n","\n","    def transform(self, X):\n","        def document_vector(doc):\n","            doc = [word for word in doc.split() if word in self.model.wv] # Split the tokenized words into a list of words\n","            if len(doc) == 0:\n","                return np.zeros(self.vector_size)\n","            return np.mean(self.model.wv[doc], axis=0)\n","        return np.vstack(X['tokenized_words'].apply(document_vector))"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"nvhlx44zvMRt","executionInfo":{"status":"ok","timestamp":1745839990623,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"55c86fa5-1237-474e-cb25-43f8a5645184"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","LOGISTIC REGRESSION WORD2VEC\n"]}]},{"cell_type":"code","source":["# Pipeline for Word2Vec + Logistic Regression\n","w2v_pipe = make_pipeline(\n","    ColumnTransformer(remainder='passthrough',\n","                      transformers=[('word2vec',\n","                                     Word2VecVectorizer(),\n","                                     ['tokenized_words'])]),\n","    StandardScaler(),\n","    LogisticRegression(penalty='l2',\n","                       solver='saga',\n","                       max_iter=1000,\n","                       random_state=229,\n","                       n_jobs=-1))\n","\n","parameters = {\n","    'logisticregression__C': (10, 1, 0.01, 0.001)\n","}\n","\n","gs_w2v_pipe = GridSearchCV(w2v_pipe,\n","                           parameters,\n","                           cv=ShuffleSplit(n_splits=1,\n","                                           test_size=0.13,\n","                                           random_state=229))\n","gs_w2v_pipe.fit(X_train, y_train)\n","print(gs_w2v_pipe.cv_results_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnr6oWod9O1E","executionInfo":{"status":"ok","timestamp":1745840047904,"user_tz":-330,"elapsed":57280,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"4a21ea99-c2aa-43ac-ca14-f45f394b3812"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'mean_fit_time': array([25.80853558, 11.8149097 ,  7.01821661,  4.90150809]), 'std_fit_time': array([0., 0., 0., 0.]), 'mean_score_time': array([0.21958208, 0.12714624, 0.14819789, 0.1260345 ]), 'std_score_time': array([0., 0., 0., 0.]), 'param_logisticregression__C': masked_array(data=[10.0, 1.0, 0.01, 0.001],\n","             mask=[False, False, False, False],\n","       fill_value=1e+20), 'params': [{'logisticregression__C': 10}, {'logisticregression__C': 1}, {'logisticregression__C': 0.01}, {'logisticregression__C': 0.001}], 'split0_test_score': array([0.62264151, 0.62735849, 0.625     , 0.64386792]), 'mean_test_score': array([0.62264151, 0.62735849, 0.625     , 0.64386792]), 'std_test_score': array([0., 0., 0., 0.]), 'rank_test_score': array([4, 2, 3, 1], dtype=int32)}\n"]}]},{"cell_type":"code","source":["print(gs_w2v_pipe.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qbtgxUl-McC","executionInfo":{"status":"ok","timestamp":1745840047922,"user_tz":-330,"elapsed":15,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"c87bc74d-6f5a-458d-a6ad-77cce0730160"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["{'logisticregression__C': 0.001}\n"]}]},{"cell_type":"code","source":["#predictions\n","predictions = gs_w2v_pipe.predict(X_test)\n","predictions = list(map(round,predictions))\n","print(classification_report(y_test, predictions))\n","print(confusion_matrix(y_test, predictions))\n","print(roc_auc_score(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kPzmGe-9VUV","executionInfo":{"status":"ok","timestamp":1745840048318,"user_tz":-330,"elapsed":393,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"2d2c941e-e753-4046-f5ac-950345c510ae"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.61      0.72      1214\n","           1       0.28      0.63      0.39       286\n","\n","    accuracy                           0.62      1500\n","   macro avg       0.58      0.62      0.55      1500\n","weighted avg       0.76      0.62      0.66      1500\n","\n","[[743 471]\n"," [105 181]]\n","0.6224467460052304\n"]}]},{"cell_type":"code","source":["# Feature importance for word2vec (vector coefficients)\n","coefficients = gs_w2v_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n","num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n","sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n","# print(len(sorted_ind))\n","# print(np.take(coefficients,sorted_ind.tolist()))\n","print(f\"Number of features (word2vec vector size): {len(coefficients)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PKCsBBW9Xtn","executionInfo":{"status":"ok","timestamp":1745840048336,"user_tz":-330,"elapsed":14,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"ba1629da-7399-4c6d-86be-61f772d6198f"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of features (word2vec vector size): 112\n"]}]},{"cell_type":"code","source":["# the top 10 features\n","top_10_indices = np.argsort(np.abs(coefficients))[-10:]  # Indices of top 10 features\n","print(\"Top 10 feature indices:\", top_10_indices)\n","print(\"Top 10 coefficients:\", coefficients[top_10_indices])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLu-d4W_AJPL","executionInfo":{"status":"ok","timestamp":1745840048364,"user_tz":-330,"elapsed":25,"user":{"displayName":"Gauranga Kumar Baishya","userId":"09042457344042071897"}},"outputId":"03e0f53e-0446-4ff1-fa2e-3a88b208defb"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 feature indices: [109  70 108  65 106 101  30 104 105 100]\n","Top 10 coefficients: [ 0.01673677 -0.01859409 -0.02030245  0.02272412  0.02382111 -0.02463701\n"," -0.0250938   0.06562118  0.10062244  0.11947248]\n"]}]}]}