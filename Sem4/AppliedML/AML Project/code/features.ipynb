{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afriyPRN4N1Q"
   },
   "source": [
    "---\n",
    "\n",
    "#### What does this notebook, `features.py` do ? – engineering quick linguistics\n",
    "\n",
    "Takes the CSV from step 1 and enriches each row with lightweight text statistics, then writes `tokenized_reviews.csv`.\n",
    "\n",
    "1. **Rating difference** – `rating_diff = user_rating – avg_rating` (did the reviewer buck consensus?).   \n",
    "2. **Quotation flag** – Does the text contain a double quote `\"...\"`?  (`quote = 1/0`).  \n",
    "3. **Tokenisation** –  \n",
    "   * Sentence split, word split, lower-case, keep only alphabetic.  \n",
    "   * Counts for `num_words`, `avg_sent_len` (words ÷ sentences), `avg_word_len`.   \n",
    "4. **Part-of-speech fractions** – run `nltk.pos_tag`, compute % of words that are verbs (`pct_verbs`), nouns, adjectives/adverbs.  \n",
    "5. **Sentiment** – VADER compound score averaged across sentences.  \n",
    "6. **Stop-word removal + lemmatisation** – store the cleaned tokens (needed later for BOW/TF-IDF).  \n",
    "7. **Save** – both numeric columns and the token list go to `tokenized_reviews.csv`.\n",
    "\n",
    "*Micro-example* (after feature step)  \n",
    "\n",
    "| num_words | sentiment | pct_verbs | quote |\n",
    "|-----------|-----------|-----------|-------|\n",
    "| 220 | –0.14 | 0.12 | 1 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from swifter) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from swifter) (6.1.1)\n",
      "Collecting dask>=2.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask-2025.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from swifter) (4.67.1)\n",
      "Requirement already satisfied: click>=8.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
      "Collecting partd>=1.4.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
      "Collecting toolz>=0.10.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Using cached toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from dask[dataframe]>=2.10.0->swifter) (19.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2025.1)\n",
      "Collecting locket (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
      "Downloading dask-2025.4.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Using cached toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16588 sha256=20d1259c6e17dd481850ad61f8218eb790690b15981d47aa0bd196473823525c\n",
      "  Stored in directory: /Users/vasu/Library/Caches/pip/wheels/d9/31/ff/ff51141a088571a9f672449e5aad5ea8bb35ca5d95ba135f30\n",
      "Successfully built swifter\n",
      "Installing collected packages: toolz, locket, partd, dask, swifter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [swifter]m3/5\u001b[0m [dask]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dask-2025.4.1 locket-1.0.0 partd-1.4.2 swifter-1.4.0 toolz-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5578,
     "status": "ok",
     "timestamp": 1745769275405,
     "user": {
      "displayName": "Nikita Kumari",
      "userId": "00030470459433194813"
     },
     "user_tz": -330
    },
    "id": "INWsTd4ayz3w",
    "outputId": "33168424-58a2-494b-adea-8cc1e95df2ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vasu/AjrVasu/Coding/iit/degree/AppliedMachineLearning/envaml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/vasu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger') # Download the missing resource\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G1Vc8k_k3Fu1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [02:23<00:00, 11895.54it/s]\n",
      "Pandas Apply: 100%|██████████████████████████████| 1710198/1710198 [00:00<00:00, 3119307.24it/s]\n",
      "Pandas Apply: 100%|█████████████████████████████████| 1710198/1710198 [12:38<00:00, 2253.77it/s]\n",
      "Pandas Apply: 100%|██████████████████████████████| 1710198/1710198 [00:00<00:00, 2746004.24it/s]\n",
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [00:28<00:00, 59283.42it/s]\n",
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [1:17:26<00:00, 368.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in: 5593.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# read review data\n",
    "data = pd.read_csv(\"../data/filtered_reviews.csv\")\n",
    "data = data.drop(columns=['book_id','ratings_count','review_likes','like_share'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# difference between user rating and average book rating\n",
    "data[\"rating_diff\"] = data[\"user_rating\"]-data[\"avg_rating\"]\n",
    "data = data.drop(columns=['avg_rating'])\n",
    "\n",
    "# flag if review contains a quotation\n",
    "data[\"quote\"] = data[\"review_text\"].str.contains(\"\\\"\")\n",
    "\n",
    "# tokenize for review length (num words), avg sentence length, avg word length\n",
    "data[\"tokenized_sents\"] = data[\"review_text\"].swifter.apply(nltk.tokenize.sent_tokenize) \n",
    "data[\"num_sentences\"] = data[\"tokenized_sents\"].swifter.apply(len)                         \n",
    "data[\"tokenized_words\"] = data[\"review_text\"].swifter.progress_bar(True).apply(lambda review: \n",
    "                                                                               [word.lower() for word in nltk.tokenize.word_tokenize(review) \n",
    "                                                                                if word.isalpha()])\n",
    "data[\"num_words\"] = data[\"tokenized_words\"].swifter.apply(len)                             \n",
    "data[\"avg_sent_len\"] = data[\"num_words\"]/data[\"num_sentences\"]\n",
    "data[\"num_letters\"] = data[\"tokenized_words\"].swifter.apply(lambda review: len([letter for word in review for letter in word]))  \n",
    "data[\"avg_word_len\"] = data[\"num_letters\"]/data[\"num_words\"]\n",
    "data = data.drop(columns=['review_text','num_sentences','num_letters'])\n",
    "\n",
    "# part of speech tagging\n",
    "data[\"pos_tags\"] = data[\"tokenized_words\"].swifter.apply(nltk.pos_tag) \n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>popular</th>\n",
       "      <th>rating_diff</th>\n",
       "      <th>quote</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>True</td>\n",
       "      <td>[This is a special book., It started slow for ...</td>\n",
       "      <td>[this, is, a, special, book, it, started, slow...</td>\n",
       "      <td>354</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>4.618644</td>\n",
       "      <td>[(this, DT), (is, VBZ), (a, DT), (special, JJ)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[A fun, fast paced science fiction thriller., ...</td>\n",
       "      <td>[a, fun, fast, paced, science, fiction, thrill...</td>\n",
       "      <td>458</td>\n",
       "      <td>13.085714</td>\n",
       "      <td>4.303493</td>\n",
       "      <td>[(a, DT), (fun, NN), (fast, RB), (paced, VBD),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>406</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>True</td>\n",
       "      <td>[A fascinating book about community and belong...</td>\n",
       "      <td>[a, fascinating, book, about, community, and, ...</td>\n",
       "      <td>494</td>\n",
       "      <td>27.444444</td>\n",
       "      <td>4.757085</td>\n",
       "      <td>[(a, DT), (fascinating, JJ), (book, NN), (abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>5</td>\n",
       "      <td>534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>True</td>\n",
       "      <td>[I haven't read a ton of \"history of the world...</td>\n",
       "      <td>[i, have, read, a, ton, of, history, of, the, ...</td>\n",
       "      <td>684</td>\n",
       "      <td>19.542857</td>\n",
       "      <td>4.771930</td>\n",
       "      <td>[(i, NNS), (have, VBP), (read, VBN), (a, DT), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>669</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>True</td>\n",
       "      <td>[A beautiful story., It is rare to encounter a...</td>\n",
       "      <td>[a, beautiful, story, it, is, rare, to, encoun...</td>\n",
       "      <td>253</td>\n",
       "      <td>18.071429</td>\n",
       "      <td>3.897233</td>\n",
       "      <td>[(a, DT), (beautiful, JJ), (story, NN), (it, P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  user_reviews  user_rating  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d           218            5   \n",
       "1  8842281e1d1347389f2ab93d60773d4d           218            3   \n",
       "2  8842281e1d1347389f2ab93d60773d4d           218            4   \n",
       "3  8842281e1d1347389f2ab93d60773d4d           218            5   \n",
       "4  8842281e1d1347389f2ab93d60773d4d           218            4   \n",
       "\n",
       "   days_since_review  popular  rating_diff  quote  \\\n",
       "0                 96        1         0.99   True   \n",
       "1                353        1        -1.10   True   \n",
       "2                406        1        -0.04   True   \n",
       "3                534        1         0.57   True   \n",
       "4                669        1        -0.31   True   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [This is a special book., It started slow for ...   \n",
       "1  [A fun, fast paced science fiction thriller., ...   \n",
       "2  [A fascinating book about community and belong...   \n",
       "3  [I haven't read a ton of \"history of the world...   \n",
       "4  [A beautiful story., It is rare to encounter a...   \n",
       "\n",
       "                                     tokenized_words  num_words  avg_sent_len  \\\n",
       "0  [this, is, a, special, book, it, started, slow...        354     17.700000   \n",
       "1  [a, fun, fast, paced, science, fiction, thrill...        458     13.085714   \n",
       "2  [a, fascinating, book, about, community, and, ...        494     27.444444   \n",
       "3  [i, have, read, a, ton, of, history, of, the, ...        684     19.542857   \n",
       "4  [a, beautiful, story, it, is, rare, to, encoun...        253     18.071429   \n",
       "\n",
       "   avg_word_len                                           pos_tags  \n",
       "0      4.618644  [(this, DT), (is, VBZ), (a, DT), (special, JJ)...  \n",
       "1      4.303493  [(a, DT), (fun, NN), (fast, RB), (paced, VBD),...  \n",
       "2      4.757085  [(a, DT), (fascinating, JJ), (book, NN), (abou...  \n",
       "3      4.771930  [(i, NNS), (have, VBP), (read, VBN), (a, DT), ...  \n",
       "4      3.897233  [(a, DT), (beautiful, JJ), (story, NN), (it, P...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "error",
     "timestamp": 1745769323589,
     "user": {
      "displayName": "Nikita Kumari",
      "userId": "00030470459433194813"
     },
     "user_tz": -330
    },
    "id": "zWPW2w5CLlHW",
    "outputId": "2a58d9b5-de72-4218-9f51-3aad7b923cc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [00:44<00:00, 38452.60it/s]\n",
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [00:25<00:00, 66540.89it/s]\n",
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [00:27<00:00, 62315.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# pos tags\n",
    "def count_pos(pos_tags, pos):\n",
    "    counts = 0\n",
    "    for word, tag in pos_tags:\n",
    "        if tag and tag[0] in pos:\n",
    "            counts += 1\n",
    "    return counts\n",
    "\n",
    "data[\"verbs\"] = data[\"pos_tags\"].swifter.progress_bar(True).apply(count_pos, pos=[\"V\"])\n",
    "data[\"pct_verbs\"] = data[\"verbs\"] / data[\"num_words\"]\n",
    "data[\"nouns\"] = data[\"pos_tags\"].swifter.progress_bar(True).apply(count_pos, pos=[\"N\"])\n",
    "data[\"pct_nouns\"] = data[\"nouns\"] / data[\"num_words\"]\n",
    "data[\"adj\"] = data[\"pos_tags\"].swifter.progress_bar(True).apply(count_pos, pos=[\"J\", \"R\"])\n",
    "data[\"pct_adj\"] = data[\"adj\"] / data[\"num_words\"]\n",
    "data = data.drop(columns=['pos_tags', 'verbs', 'nouns', 'adj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SZaAizFOaP9J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|████████████████████████████████| 1710198/1710198 [1:44:16<00:00, 273.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "def review_sentiment(review_sents):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    comptot = 0\n",
    "    for sentence in review_sents:\n",
    "        scores = sid.polarity_scores(sentence)\n",
    "        comptot += scores['compound']\n",
    "    return comptot / len(review_sents)\n",
    "\n",
    "# Apply sentiment analysis in parallel with progress bar\n",
    "data[\"sentiment\"] = data[\"tokenized_sents\"].swifter.progress_bar(True).apply(review_sentiment)\n",
    "\n",
    "# Drop the tokenized sentences column\n",
    "data = data.drop(columns=['tokenized_sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>popular</th>\n",
       "      <th>rating_diff</th>\n",
       "      <th>quote</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>pct_verbs</th>\n",
       "      <th>pct_nouns</th>\n",
       "      <th>pct_adj</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>True</td>\n",
       "      <td>[this, is, a, special, book, it, started, slow...</td>\n",
       "      <td>354</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>4.618644</td>\n",
       "      <td>0.192090</td>\n",
       "      <td>0.225989</td>\n",
       "      <td>0.163842</td>\n",
       "      <td>0.123650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>353</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>True</td>\n",
       "      <td>[a, fun, fast, paced, science, fiction, thrill...</td>\n",
       "      <td>458</td>\n",
       "      <td>13.085714</td>\n",
       "      <td>4.303493</td>\n",
       "      <td>0.209607</td>\n",
       "      <td>0.235808</td>\n",
       "      <td>0.120087</td>\n",
       "      <td>0.096211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>406</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>True</td>\n",
       "      <td>[a, fascinating, book, about, community, and, ...</td>\n",
       "      <td>494</td>\n",
       "      <td>27.444444</td>\n",
       "      <td>4.757085</td>\n",
       "      <td>0.155870</td>\n",
       "      <td>0.275304</td>\n",
       "      <td>0.147773</td>\n",
       "      <td>0.047972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>5</td>\n",
       "      <td>534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>True</td>\n",
       "      <td>[i, have, read, a, ton, of, history, of, the, ...</td>\n",
       "      <td>684</td>\n",
       "      <td>19.542857</td>\n",
       "      <td>4.771930</td>\n",
       "      <td>0.198830</td>\n",
       "      <td>0.248538</td>\n",
       "      <td>0.141813</td>\n",
       "      <td>0.220063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>669</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>True</td>\n",
       "      <td>[a, beautiful, story, it, is, rare, to, encoun...</td>\n",
       "      <td>253</td>\n",
       "      <td>18.071429</td>\n",
       "      <td>3.897233</td>\n",
       "      <td>0.209486</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.114625</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  user_reviews  user_rating  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d           218            5   \n",
       "1  8842281e1d1347389f2ab93d60773d4d           218            3   \n",
       "2  8842281e1d1347389f2ab93d60773d4d           218            4   \n",
       "3  8842281e1d1347389f2ab93d60773d4d           218            5   \n",
       "4  8842281e1d1347389f2ab93d60773d4d           218            4   \n",
       "\n",
       "   days_since_review  popular  rating_diff  quote  \\\n",
       "0                 96        1         0.99   True   \n",
       "1                353        1        -1.10   True   \n",
       "2                406        1        -0.04   True   \n",
       "3                534        1         0.57   True   \n",
       "4                669        1        -0.31   True   \n",
       "\n",
       "                                     tokenized_words  num_words  avg_sent_len  \\\n",
       "0  [this, is, a, special, book, it, started, slow...        354     17.700000   \n",
       "1  [a, fun, fast, paced, science, fiction, thrill...        458     13.085714   \n",
       "2  [a, fascinating, book, about, community, and, ...        494     27.444444   \n",
       "3  [i, have, read, a, ton, of, history, of, the, ...        684     19.542857   \n",
       "4  [a, beautiful, story, it, is, rare, to, encoun...        253     18.071429   \n",
       "\n",
       "   avg_word_len  pct_verbs  pct_nouns   pct_adj  sentiment  \n",
       "0      4.618644   0.192090   0.225989  0.163842   0.123650  \n",
       "1      4.303493   0.209607   0.235808  0.120087   0.096211  \n",
       "2      4.757085   0.155870   0.275304  0.147773   0.047972  \n",
       "3      4.771930   0.198830   0.248538  0.141813   0.220063  \n",
       "4      3.897233   0.209486   0.205534  0.114625   0.068500  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dEM-uibiaVT6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|█████████████████████████████████| 1710198/1710198 [05:50<00:00, 4874.39it/s]\n",
      "Pandas Apply: 100%|█████████████████████████████████| 1710198/1710198 [04:24<00:00, 6461.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in: 795.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# further text processing\n",
    "# remove stop words\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "data[\"tokenized_words\"] = data[\"tokenized_words\"].swifter.progress_bar(True).apply(lambda review: \n",
    "                                                                                   [word for word in review if word not in stopwords])\n",
    "\n",
    "# lemmatization\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "data[\"tokenized_words\"] = data[\"tokenized_words\"].swifter.progress_bar(True).apply(lambda review: \n",
    "                                                                                   [wnl.lemmatize(word) for word in review])\n",
    "\n",
    "# reorder columns\n",
    "data = data[[\"popular\", \"user_reviews\", \"days_since_review\", \"user_rating\", \"rating_diff\",\n",
    "             \"num_words\", \"avg_word_len\", \"avg_sent_len\", \"pct_verbs\", \"pct_nouns\", \"pct_adj\",\n",
    "             \"quote\", \"sentiment\", \"tokenized_words\"]]\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jjx08PD03Ug-"
   },
   "outputs": [],
   "source": [
    "# save dataset\n",
    "save_path = '../data/tokenized_reviews.csv'\n",
    "data.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9KGvM4w93V4i"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1710198, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MwEYwasV3x_6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popular</th>\n",
       "      <th>user_reviews</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>rating_diff</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>pct_verbs</th>\n",
       "      <th>pct_nouns</th>\n",
       "      <th>pct_adj</th>\n",
       "      <th>quote</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1159061</th>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>654</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>14</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['wow', 'really', 'limit', 'crap', 'people', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194985</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>956</td>\n",
       "      <td>5</td>\n",
       "      <td>1.22</td>\n",
       "      <td>416</td>\n",
       "      <td>4.293269</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.204327</td>\n",
       "      <td>0.206731</td>\n",
       "      <td>0.161058</td>\n",
       "      <td>True</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>['top', 'six', 'reason', 'v', 'virgin', 'suck'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872076</th>\n",
       "      <td>1</td>\n",
       "      <td>557</td>\n",
       "      <td>814</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>16</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>['quite', 'good', 'first', 'four', 'still', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639315</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1017</td>\n",
       "      <td>5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>73</td>\n",
       "      <td>4.287671</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>['brilliantly', 'written', 'book', 'well', 'cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380611</th>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>1117</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>161</td>\n",
       "      <td>3.763975</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>0.204969</td>\n",
       "      <td>0.223602</td>\n",
       "      <td>0.161491</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>['favorite', 'book', 'ever', 'think', 'writing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         popular  user_reviews  days_since_review  user_rating  rating_diff  \\\n",
       "1159061        1           206                654            0        -3.00   \n",
       "1194985        1           256                956            5         1.22   \n",
       "872076         1           557                814            4        -0.43   \n",
       "1639315        0            34               1017            5         0.93   \n",
       "1380611        0           197               1117            3        -0.86   \n",
       "\n",
       "         num_words  avg_word_len  avg_sent_len  pct_verbs  pct_nouns  \\\n",
       "1159061         14      3.785714      4.666667   0.142857   0.357143   \n",
       "1194985        416      4.293269     52.000000   0.204327   0.206731   \n",
       "872076          16      3.812500     16.000000   0.000000   0.125000   \n",
       "1639315         73      4.287671     12.166667   0.178082   0.232877   \n",
       "1380611        161      3.763975     32.200000   0.204969   0.223602   \n",
       "\n",
       "          pct_adj  quote  sentiment  \\\n",
       "1159061  0.142857  False     0.0000   \n",
       "1194985  0.161058   True     0.3104   \n",
       "872076   0.437500  False     0.0243   \n",
       "1639315  0.205479  False     0.3200   \n",
       "1380611  0.161491  False     0.0614   \n",
       "\n",
       "                                           tokenized_words  \n",
       "1159061  ['wow', 'really', 'limit', 'crap', 'people', '...  \n",
       "1194985  ['top', 'six', 'reason', 'v', 'virgin', 'suck'...  \n",
       "872076   ['quite', 'good', 'first', 'four', 'still', 's...  \n",
       "1639315  ['brilliantly', 'written', 'book', 'well', 'cr...  \n",
       "1380611  ['favorite', 'book', 'ever', 'think', 'writing...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
